{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/eitan.c@staff.technion.ac.il/ECG_proj/dockvenv/code/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-eitan.c@staff.technion.ac.il'\n",
      "2023-05-06 21:34:55.137261: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-06 21:35:04.426361: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-06 21:35:04.428692: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-06 21:35:04.428709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os              : Linux-5.15.0-69-generic-x86_64-with-glibc2.29\n",
      "python          : 3.8.5\n",
      "tsai            : 0.3.6\n",
      "fastai          : 2.7.12\n",
      "fastcore        : 1.5.29\n",
      "torch           : 1.13.1+cu117\n",
      "device          : 1 gpu (['NVIDIA GeForce RTX 3090'])\n",
      "cpu cores       : 8\n",
      "threads per cpu : 2\n",
      "RAM             : 62.71 GB\n",
      "GPU memory      : [24.0] GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "from tsai.all import *\n",
    "import sklearn.metrics as skm\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# import outsiders\n",
    "sys.path.append('..')\n",
    "import data_preprocessing\n",
    "my_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14595, 2560)\n",
      "<class 'numpy.ndarray'>\n",
      "(14595,)\n",
      "(14595,)\n",
      "(14595,)\n",
      "(14595, 1, 2560)\n",
      "(14595, 1, 2560)\n",
      "<class 'numpy.memmap'>\n",
      "(14595,)\n",
      "<class 'numpy.memmap'>\n",
      "13135\n",
      "1460\n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "def split_data_indexes(len_data, train_ratio):\n",
    "    indexes = list(range(len_data))\n",
    "    random.shuffle(indexes)\n",
    "    train_size = int(len_data * train_ratio)\n",
    "    train_indexes = indexes[:train_size]\n",
    "    test_indexes = indexes[train_size:]\n",
    "    return (train_indexes, test_indexes)\n",
    "# ssh remote\n",
    "swell_data = data_preprocessing.load_data(os.path.join(\"../swell_dataset\", \"swell_dict_index_6.npy\")) \n",
    "swell_data = data_preprocessing.swell_prepare_for_10fold(swell_data)\n",
    "swell_data[:, 1:4] -= 1\n",
    "x = swell_data[:, 4:]\n",
    "\n",
    "y_arousal = swell_data[:, 2]\n",
    "y_arousal = y_arousal.reshape(y_arousal.shape[0],)\n",
    "\n",
    "y_valence = swell_data[:, 3]\n",
    "y_valence = y_valence.reshape(y_valence.shape[0],)\n",
    "\n",
    "y_stress = swell_data[:, 1]\n",
    "y_stress = y_stress.reshape(y_stress.shape[0],)\n",
    "\n",
    "print(x.shape)\n",
    "print(type(x))\n",
    "print(y_stress.shape)\n",
    "print(y_valence.shape)\n",
    "print(y_arousal.shape)\n",
    "\n",
    "\n",
    "# set X to 3-Dim array\n",
    "x = np.array(x)\n",
    "x = np.expand_dims(x, axis=1)\n",
    "print(x.shape)\n",
    "filename = 'X_memmap.npy'\n",
    "X = np.memmap(filename, dtype='float32', mode='w+', shape=x.shape)\n",
    "X[:] = x[:]\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "\n",
    "filename = 'Y_arousal_memmap.npy'\n",
    "Y_arousal = np.memmap(filename, dtype='float32', mode='w+', shape=y_arousal.shape)\n",
    "Y_arousal[:] = y_arousal[:]\n",
    "\n",
    "filename = 'Y_valence_memmap.npy'\n",
    "Y_valence = np.memmap(filename, dtype='float32', mode='w+', shape=y_valence.shape)\n",
    "Y_valence[:] = y_valence[:]\n",
    "\n",
    "filename = 'Y_stress_memmap.npy'\n",
    "Y_stress = np.memmap(filename, dtype='float32', mode='w+', shape=y_stress.shape)\n",
    "Y_stress[:] = y_stress[:]\n",
    "print(Y_stress.shape)\n",
    "print(type(Y_stress))\n",
    "splits=split_data_indexes(X.shape[0],0.9)\n",
    "print(len(splits[0]))\n",
    "print(len(splits[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "0.0\n",
      "8.0\n",
      "0.0\n",
      "8.0\n",
      "1.0\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[3. 3. 3. ... 1. 1. 1.]\n",
      "[5. 5. 5. ... 6. 6. 6.]\n"
     ]
    },
    {
     "data": {
      "text/plain": "2.0"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.max(y_stress))\n",
    "print(np.min(y_stress))\n",
    "print(np.max(y_arousal))\n",
    "print(np.min(y_arousal)) \n",
    "print(np.max(y_valence))\n",
    "print(np.min(y_valence)) \n",
    "print(Y_stress)\n",
    "print(Y_arousal)\n",
    "print(Y_valence)\n",
    "np.max(Y_stress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare dataset loaders and model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Path('models/S0.pth')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfms  = [None, [Categorize()]]\n",
    "splits=np.load('./splits_B.npy',allow_pickle=True)\n",
    "splits=(splits[0],splits[1])\n",
    "dsets_arousal = TSDatasets(X, Y_arousal, tfms=tfms, splits=splits, inplace=True)\n",
    "dsets_valence = TSDatasets(X, Y_valence, tfms=tfms, splits=splits, inplace=True)\n",
    "dsets_stress = TSDatasets(X, Y_stress, tfms=tfms, splits=splits, inplace=True)\n",
    "\n",
    "bs=128\n",
    "dls_stress = TSDataLoaders.from_dsets(dsets_stress.train, dsets_stress.valid, bs=bs, batch_tfms=[TSStandardize()], num_workers=0)\n",
    "dls_arousal = TSDataLoaders.from_dsets(dsets_arousal.train, dsets_arousal.valid, bs=bs, batch_tfms=[TSStandardize()], num_workers=0)\n",
    "dls_valence = TSDataLoaders.from_dsets(dsets_valence.train, dsets_valence.valid, bs=bs, batch_tfms=[TSStandardize()], num_workers=0)\n",
    "\n",
    "model_stress = InceptionTime(dls_stress.vars, dls_stress.c)\n",
    "model_arousal = InceptionTime(dls_arousal.vars, dls_arousal.c)\n",
    "model_valence = InceptionTime(dls_valence.vars, dls_valence.c)\n",
    "\n",
    "learn_stress = Learner(dls_stress, model_stress, metrics=accuracy)\n",
    "learn_arousal = Learner(dls_arousal, model_arousal, metrics=accuracy)\n",
    "learn_valence = Learner(dls_valence, model_valence, metrics=accuracy)\n",
    "\n",
    "learn_stress.save('S0')\n",
    "learn_arousal.save('S0')\n",
    "learn_valence.save('S0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionTime(\n",
      "  (inceptionblock): InceptionBlock(\n",
      "    (inception): ModuleList(\n",
      "      (0): InceptionModule(\n",
      "        (convs): ModuleList(\n",
      "          (0): Conv1d(1, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
      "          (1): Conv1d(1, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
      "          (2): Conv1d(1, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "        )\n",
      "        (maxconvpool): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "          (1): Conv1d(1, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "        (concat): Concat(dim=1)\n",
      "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (1): InceptionModule(\n",
      "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convs): ModuleList(\n",
      "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
      "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
      "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "        )\n",
      "        (maxconvpool): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "        (concat): Concat(dim=1)\n",
      "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (2): InceptionModule(\n",
      "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convs): ModuleList(\n",
      "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
      "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
      "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "        )\n",
      "        (maxconvpool): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "        (concat): Concat(dim=1)\n",
      "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (3): InceptionModule(\n",
      "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convs): ModuleList(\n",
      "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
      "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
      "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "        )\n",
      "        (maxconvpool): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "        (concat): Concat(dim=1)\n",
      "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (4): InceptionModule(\n",
      "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convs): ModuleList(\n",
      "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
      "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
      "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "        )\n",
      "        (maxconvpool): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "        (concat): Concat(dim=1)\n",
      "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "      (5): InceptionModule(\n",
      "        (bottleneck): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (convs): ModuleList(\n",
      "          (0): Conv1d(32, 32, kernel_size=(39,), stride=(1,), padding=(19,), bias=False)\n",
      "          (1): Conv1d(32, 32, kernel_size=(19,), stride=(1,), padding=(9,), bias=False)\n",
      "          (2): Conv1d(32, 32, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)\n",
      "        )\n",
      "        (maxconvpool): Sequential(\n",
      "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "          (1): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "        (concat): Concat(dim=1)\n",
      "        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (shortcut): ModuleList(\n",
      "      (0): ConvBlock(\n",
      "        (0): Conv1d(1, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (add): Add\n",
      "    (act): ReLU()\n",
      "  )\n",
      "  (gap): GAP1d(\n",
      "    (gap): AdaptiveAvgPool1d(output_size=1)\n",
      "    (flatten): Reshape(bs)\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_stress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train for stress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "SuggestedLRs(valley=0.0020892962347716093)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_stress.load('S0')\n",
    "learn_stress.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "\n    <div>\n      <progress value='4' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      4.00% [4/100 00:52&lt;20:59]\n    </div>\n    \n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.432568</td>\n      <td>1.504460</td>\n      <td>0.257890</td>\n      <td>00:24</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.232076</td>\n      <td>1.461440</td>\n      <td>0.305227</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.112743</td>\n      <td>1.405773</td>\n      <td>0.310651</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.055276</td>\n      <td>1.436983</td>\n      <td>0.248028</td>\n      <td>00:09</td>\n    </tr>\n  </tbody>\n</table><p>\n\n    <div>\n      <progress value='5' class='' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      31.25% [5/16 00:00&lt;00:00 1.0220]\n    </div>\n    ",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_stress.fit_one_cycle(100,lr_max=1e-3)\n",
    "learn_stress.save('S1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_stress.save_all(path='./stress_inc', dls_fname='dls_stress', model_fname='model_stress', learner_fname='learner_stress')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train for arousal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "SuggestedLRs(valley=0.0003311311302240938)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_arousal.load('S0')\n",
    "learn_arousal.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.197161</td>\n      <td>2.114778</td>\n      <td>0.167123</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2.074747</td>\n      <td>2.014642</td>\n      <td>0.234932</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.989311</td>\n      <td>1.928762</td>\n      <td>0.287671</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.910060</td>\n      <td>1.860339</td>\n      <td>0.332192</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.843829</td>\n      <td>1.806120</td>\n      <td>0.339041</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.801920</td>\n      <td>1.758067</td>\n      <td>0.344521</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.771609</td>\n      <td>1.716871</td>\n      <td>0.343151</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.742252</td>\n      <td>1.690797</td>\n      <td>0.346575</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.720527</td>\n      <td>1.675670</td>\n      <td>0.366438</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.683166</td>\n      <td>1.627590</td>\n      <td>0.348630</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.634624</td>\n      <td>1.587314</td>\n      <td>0.374658</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.598103</td>\n      <td>1.562054</td>\n      <td>0.346575</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.544250</td>\n      <td>1.472746</td>\n      <td>0.422603</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.497482</td>\n      <td>1.461308</td>\n      <td>0.458219</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.446370</td>\n      <td>1.399810</td>\n      <td>0.450000</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.413737</td>\n      <td>1.353976</td>\n      <td>0.465068</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.357049</td>\n      <td>1.304779</td>\n      <td>0.495890</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>1.298995</td>\n      <td>1.283407</td>\n      <td>0.527397</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.240239</td>\n      <td>1.228769</td>\n      <td>0.540411</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.176745</td>\n      <td>1.155572</td>\n      <td>0.521233</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.109343</td>\n      <td>1.106644</td>\n      <td>0.623288</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.042716</td>\n      <td>0.997047</td>\n      <td>0.646575</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.976233</td>\n      <td>0.970882</td>\n      <td>0.639726</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.896435</td>\n      <td>0.855926</td>\n      <td>0.726027</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.861519</td>\n      <td>0.734279</td>\n      <td>0.752055</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.802579</td>\n      <td>0.773558</td>\n      <td>0.747260</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.736796</td>\n      <td>0.698115</td>\n      <td>0.763699</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.689695</td>\n      <td>0.699501</td>\n      <td>0.743151</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.659158</td>\n      <td>0.740559</td>\n      <td>0.746575</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.612327</td>\n      <td>0.640574</td>\n      <td>0.750000</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.587807</td>\n      <td>0.787709</td>\n      <td>0.712329</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.557997</td>\n      <td>0.569906</td>\n      <td>0.792466</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.523385</td>\n      <td>0.514164</td>\n      <td>0.813699</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.514412</td>\n      <td>0.578485</td>\n      <td>0.783562</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.474325</td>\n      <td>0.419117</td>\n      <td>0.857534</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.462802</td>\n      <td>0.466425</td>\n      <td>0.829452</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.451658</td>\n      <td>0.433903</td>\n      <td>0.834247</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.443334</td>\n      <td>0.639069</td>\n      <td>0.728082</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.402857</td>\n      <td>0.400386</td>\n      <td>0.856164</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.404472</td>\n      <td>0.347500</td>\n      <td>0.862329</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.385592</td>\n      <td>0.441374</td>\n      <td>0.825342</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.363857</td>\n      <td>0.435354</td>\n      <td>0.819178</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.360192</td>\n      <td>0.355202</td>\n      <td>0.880137</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.344239</td>\n      <td>0.351583</td>\n      <td>0.867123</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.337912</td>\n      <td>0.427795</td>\n      <td>0.839726</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.333258</td>\n      <td>0.327669</td>\n      <td>0.861644</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.326815</td>\n      <td>0.342494</td>\n      <td>0.868493</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.312006</td>\n      <td>0.273263</td>\n      <td>0.894521</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.308656</td>\n      <td>0.411411</td>\n      <td>0.854110</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.300211</td>\n      <td>0.288927</td>\n      <td>0.891096</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.302203</td>\n      <td>0.332844</td>\n      <td>0.852055</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.292966</td>\n      <td>0.283955</td>\n      <td>0.886986</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.289811</td>\n      <td>0.284793</td>\n      <td>0.903425</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.275533</td>\n      <td>0.263653</td>\n      <td>0.890411</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.273112</td>\n      <td>0.401793</td>\n      <td>0.858219</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.252081</td>\n      <td>0.235412</td>\n      <td>0.911644</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.246144</td>\n      <td>0.278236</td>\n      <td>0.873973</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.260215</td>\n      <td>0.256008</td>\n      <td>0.896575</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.239496</td>\n      <td>0.236902</td>\n      <td>0.895205</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.238491</td>\n      <td>0.240683</td>\n      <td>0.913699</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.232210</td>\n      <td>0.301759</td>\n      <td>0.882877</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.230276</td>\n      <td>0.220119</td>\n      <td>0.928082</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.219659</td>\n      <td>0.211472</td>\n      <td>0.930822</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.224244</td>\n      <td>0.327418</td>\n      <td>0.895205</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.215803</td>\n      <td>0.213010</td>\n      <td>0.918493</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.205787</td>\n      <td>0.186120</td>\n      <td>0.942466</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.202172</td>\n      <td>0.230122</td>\n      <td>0.931507</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.202596</td>\n      <td>0.198220</td>\n      <td>0.917123</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.189484</td>\n      <td>0.167022</td>\n      <td>0.943836</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.191966</td>\n      <td>0.166086</td>\n      <td>0.936986</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.183161</td>\n      <td>0.178790</td>\n      <td>0.934247</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.186125</td>\n      <td>0.176530</td>\n      <td>0.936301</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.176429</td>\n      <td>0.155549</td>\n      <td>0.954110</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.176113</td>\n      <td>0.154865</td>\n      <td>0.939726</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.171079</td>\n      <td>0.160300</td>\n      <td>0.944521</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.168878</td>\n      <td>0.167309</td>\n      <td>0.939041</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.158984</td>\n      <td>0.149198</td>\n      <td>0.942466</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.166517</td>\n      <td>0.143891</td>\n      <td>0.961644</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.156055</td>\n      <td>0.130594</td>\n      <td>0.960274</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.154285</td>\n      <td>0.141798</td>\n      <td>0.950000</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.158926</td>\n      <td>0.135804</td>\n      <td>0.956164</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.149844</td>\n      <td>0.146603</td>\n      <td>0.950685</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.145865</td>\n      <td>0.146522</td>\n      <td>0.951370</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.149267</td>\n      <td>0.140894</td>\n      <td>0.952740</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.140494</td>\n      <td>0.119451</td>\n      <td>0.963014</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.144271</td>\n      <td>0.128015</td>\n      <td>0.957534</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.137457</td>\n      <td>0.124508</td>\n      <td>0.959589</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.137141</td>\n      <td>0.124757</td>\n      <td>0.956849</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.136701</td>\n      <td>0.113734</td>\n      <td>0.967808</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.141077</td>\n      <td>0.116191</td>\n      <td>0.967123</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.135163</td>\n      <td>0.118062</td>\n      <td>0.964384</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.130110</td>\n      <td>0.116750</td>\n      <td>0.965069</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.137491</td>\n      <td>0.114743</td>\n      <td>0.966438</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.130170</td>\n      <td>0.112846</td>\n      <td>0.969178</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.132411</td>\n      <td>0.115002</td>\n      <td>0.967808</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.125326</td>\n      <td>0.110483</td>\n      <td>0.971233</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.131196</td>\n      <td>0.113258</td>\n      <td>0.967123</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.128199</td>\n      <td>0.114618</td>\n      <td>0.967808</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.126211</td>\n      <td>0.111059</td>\n      <td>0.969178</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.126626</td>\n      <td>0.111855</td>\n      <td>0.971918</td>\n      <td>00:10</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Path('models/S1.pth')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_arousal.fit_one_cycle(100,lr_max=1e-4)\n",
    "learn_arousal.save('S1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_arousal.save_all(path='./arousal_inc', dls_fname='dls_arousal', model_fname='model_arousal', learner_fname='learner_arousal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train for valence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "SuggestedLRs(valley=0.001737800776027143)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_valence.load('S0')\n",
    "learn_valence.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.824867</td>\n      <td>1.669051</td>\n      <td>0.415753</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.667524</td>\n      <td>1.555377</td>\n      <td>0.445890</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.597406</td>\n      <td>1.515187</td>\n      <td>0.407534</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.559265</td>\n      <td>1.440574</td>\n      <td>0.456849</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.524295</td>\n      <td>1.403774</td>\n      <td>0.485616</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.500435</td>\n      <td>1.415810</td>\n      <td>0.449315</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.435993</td>\n      <td>1.328974</td>\n      <td>0.509589</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.339625</td>\n      <td>1.526630</td>\n      <td>0.421918</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.229593</td>\n      <td>1.294035</td>\n      <td>0.504795</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.099538</td>\n      <td>1.060807</td>\n      <td>0.623288</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.968087</td>\n      <td>1.038952</td>\n      <td>0.643151</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.836262</td>\n      <td>0.801161</td>\n      <td>0.648630</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.774579</td>\n      <td>1.199734</td>\n      <td>0.625342</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.676744</td>\n      <td>0.903409</td>\n      <td>0.658219</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.630095</td>\n      <td>1.018099</td>\n      <td>0.613014</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.571127</td>\n      <td>0.798287</td>\n      <td>0.647945</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.547321</td>\n      <td>1.086676</td>\n      <td>0.652740</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.512947</td>\n      <td>0.618121</td>\n      <td>0.744521</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.488987</td>\n      <td>0.488895</td>\n      <td>0.789726</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.468432</td>\n      <td>0.511895</td>\n      <td>0.753425</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.472002</td>\n      <td>0.648476</td>\n      <td>0.731507</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.470801</td>\n      <td>1.754121</td>\n      <td>0.496575</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.431382</td>\n      <td>0.513707</td>\n      <td>0.780137</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.429852</td>\n      <td>0.967471</td>\n      <td>0.706849</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.407222</td>\n      <td>0.450616</td>\n      <td>0.812329</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.383961</td>\n      <td>1.440499</td>\n      <td>0.732877</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.360089</td>\n      <td>0.356497</td>\n      <td>0.828082</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.369362</td>\n      <td>0.859518</td>\n      <td>0.756164</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.353255</td>\n      <td>0.535603</td>\n      <td>0.791781</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.333870</td>\n      <td>0.873702</td>\n      <td>0.736301</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.311518</td>\n      <td>0.352539</td>\n      <td>0.844521</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.313609</td>\n      <td>0.311603</td>\n      <td>0.864384</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.310058</td>\n      <td>0.665954</td>\n      <td>0.741781</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.302216</td>\n      <td>0.258065</td>\n      <td>0.896575</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.297067</td>\n      <td>0.356053</td>\n      <td>0.828767</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.283627</td>\n      <td>0.457311</td>\n      <td>0.854795</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.278267</td>\n      <td>0.385647</td>\n      <td>0.850685</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.257490</td>\n      <td>0.336981</td>\n      <td>0.875342</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.271564</td>\n      <td>0.260570</td>\n      <td>0.878082</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.250907</td>\n      <td>0.361452</td>\n      <td>0.847945</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.251797</td>\n      <td>0.261153</td>\n      <td>0.882877</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.247171</td>\n      <td>0.268832</td>\n      <td>0.882877</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.267176</td>\n      <td>0.358037</td>\n      <td>0.846575</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.228938</td>\n      <td>0.230793</td>\n      <td>0.900000</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.230741</td>\n      <td>0.221842</td>\n      <td>0.903425</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.227056</td>\n      <td>0.377050</td>\n      <td>0.850000</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.220643</td>\n      <td>0.310752</td>\n      <td>0.872603</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.220450</td>\n      <td>0.325130</td>\n      <td>0.878767</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.211173</td>\n      <td>0.195197</td>\n      <td>0.921233</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.198001</td>\n      <td>0.170852</td>\n      <td>0.930137</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.202108</td>\n      <td>0.218069</td>\n      <td>0.893836</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.193229</td>\n      <td>0.163756</td>\n      <td>0.930137</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.195981</td>\n      <td>0.252884</td>\n      <td>0.888356</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.188499</td>\n      <td>0.196612</td>\n      <td>0.917123</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.182438</td>\n      <td>0.206275</td>\n      <td>0.915069</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.181366</td>\n      <td>0.436992</td>\n      <td>0.829452</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.169288</td>\n      <td>0.143460</td>\n      <td>0.942466</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.169055</td>\n      <td>0.213764</td>\n      <td>0.915069</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.163666</td>\n      <td>0.153105</td>\n      <td>0.939041</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.162446</td>\n      <td>0.134176</td>\n      <td>0.952740</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.155346</td>\n      <td>0.152989</td>\n      <td>0.944521</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.144483</td>\n      <td>0.153586</td>\n      <td>0.936986</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.150690</td>\n      <td>0.123853</td>\n      <td>0.951370</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.155257</td>\n      <td>0.137463</td>\n      <td>0.952055</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.147372</td>\n      <td>0.252047</td>\n      <td>0.903425</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.141149</td>\n      <td>0.138904</td>\n      <td>0.945890</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.133409</td>\n      <td>0.122934</td>\n      <td>0.954110</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.125749</td>\n      <td>0.188011</td>\n      <td>0.922603</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.117422</td>\n      <td>0.137413</td>\n      <td>0.945205</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.120609</td>\n      <td>0.104655</td>\n      <td>0.966438</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.110418</td>\n      <td>0.099401</td>\n      <td>0.966438</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.111239</td>\n      <td>0.119263</td>\n      <td>0.951370</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.113874</td>\n      <td>0.102839</td>\n      <td>0.966438</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.101008</td>\n      <td>0.090182</td>\n      <td>0.969863</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.097886</td>\n      <td>0.091859</td>\n      <td>0.964384</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.092711</td>\n      <td>0.094362</td>\n      <td>0.960274</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.092428</td>\n      <td>0.081236</td>\n      <td>0.973288</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.089501</td>\n      <td>0.082044</td>\n      <td>0.969178</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.092140</td>\n      <td>0.083077</td>\n      <td>0.972603</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.085621</td>\n      <td>0.084249</td>\n      <td>0.971918</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.080934</td>\n      <td>0.086381</td>\n      <td>0.967123</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.079431</td>\n      <td>0.080748</td>\n      <td>0.971233</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.075683</td>\n      <td>0.069910</td>\n      <td>0.974658</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.077764</td>\n      <td>0.073569</td>\n      <td>0.974658</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.075707</td>\n      <td>0.070141</td>\n      <td>0.978767</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.068375</td>\n      <td>0.066592</td>\n      <td>0.977397</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.067159</td>\n      <td>0.068027</td>\n      <td>0.981507</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.062305</td>\n      <td>0.062575</td>\n      <td>0.980137</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.062493</td>\n      <td>0.063457</td>\n      <td>0.980822</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.064802</td>\n      <td>0.064838</td>\n      <td>0.973288</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.058714</td>\n      <td>0.065992</td>\n      <td>0.979452</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.062444</td>\n      <td>0.062254</td>\n      <td>0.982192</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.059878</td>\n      <td>0.057556</td>\n      <td>0.984932</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.055695</td>\n      <td>0.060086</td>\n      <td>0.980137</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.054535</td>\n      <td>0.059386</td>\n      <td>0.982877</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.054506</td>\n      <td>0.058777</td>\n      <td>0.982192</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.053884</td>\n      <td>0.058804</td>\n      <td>0.981507</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.054089</td>\n      <td>0.058321</td>\n      <td>0.982877</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.054779</td>\n      <td>0.059328</td>\n      <td>0.981507</td>\n      <td>00:10</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.055542</td>\n      <td>0.060202</td>\n      <td>0.982192</td>\n      <td>00:10</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Path('models/S1.pth')"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_valence.fit_one_cycle(100,lr_max=1e-3)\n",
    "learn_valence.save('S1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_valence.save_all(path='./valence_inc', dls_fname='dls_valence', model_fname='model_valence', learner_fname='learner_valence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./splits_inc_2504',splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generaliztion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_indexes_var2(ratio, objects):\n",
    "    # Split objects into train and test sets\n",
    "    num_test = int(len(set(objects)))-int(len(set(objects)) * ratio)\n",
    "    obj_list = list(set(objects))\n",
    "    random.shuffle(obj_list)\n",
    "    test_objs = obj_list[:num_test]\n",
    "    train_objs = obj_list[num_test:]\n",
    "\n",
    "    train_idxs = []\n",
    "    test_idxs = []\n",
    "\n",
    "    # Assign each object's samples to the appropriate set\n",
    "    for obj in train_objs:\n",
    "        obj_idxs = [i for i, o in enumerate(objects) if o == obj]\n",
    "        train_idxs.extend(obj_idxs)\n",
    "\n",
    "    for obj in test_objs:\n",
    "        obj_idxs = [i for i, o in enumerate(objects) if o == obj]\n",
    "        test_idxs.extend(obj_idxs)\n",
    "\n",
    "    # Shuffle the final train and test sets\n",
    "    random.shuffle(train_idxs)\n",
    "    random.shuffle(test_idxs)\n",
    "\n",
    "    return (train_idxs), (test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot len is  14595\n",
      "trin len is  13135.5\n",
      "test len is  1459.5\n",
      "train obj:\n",
      "test obj:\n",
      "{1.0, 3.0, 5.0, 7.0, 9.0, 10.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0} 12698\n",
      "{2.0, 4.0, 6.0} 1897\n"
     ]
    }
   ],
   "source": [
    "# check train_test_indexes_var2()\n",
    "# not for each runnig acieve exaclty 9:1 train:test\n",
    "ratio = 0.9\n",
    "objects=swell_data[:,0]\n",
    "print(\"tot len is \", len(objects))\n",
    "print(\"trin len is \",ratio*len(objects))\n",
    "print(\"test len is \",len(objects)-ratio*len(objects))\n",
    "i=train_test_indexes_var2(ratio, objects)\n",
    "#print(\"train idx \",i[0])\n",
    "#print(\"test idx \",i[1])\n",
    "print(\"train obj:\")\n",
    "tr=[]\n",
    "for j in i[0]:\n",
    "  #print(objects[int(j)])\n",
    "  tr.append(objects[int(j)])\n",
    "tr_u=set(tr)\n",
    "\n",
    "print(\"test obj:\")\n",
    "te=[]\n",
    "for j in i[1]:\n",
    "  #print(objects[int(j)])\n",
    "  te.append(objects[int(j)])\n",
    "te_u=set(te)\n",
    "\n",
    "print(tr_u,len(tr))\n",
    "print(te_u,len(te))\n",
    "splits=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Path('models/S0.pth')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfms  = [None, [Categorize()]]\n",
    "\n",
    "dsets_arousal = TSDatasets(X, Y_arousal, tfms=tfms, splits=splits, inplace=True)\n",
    "dsets_valence = TSDatasets(X, Y_valence, tfms=tfms, splits=splits, inplace=True)\n",
    "dsets_stress = TSDatasets(X, Y_stress, tfms=tfms, splits=splits, inplace=True)\n",
    "\n",
    "bs=128\n",
    "dls_stress = TSDataLoaders.from_dsets(dsets_stress.train, dsets_stress.valid, bs=bs, batch_tfms=[TSStandardize()], num_workers=0)\n",
    "dls_arousal = TSDataLoaders.from_dsets(dsets_arousal.train, dsets_arousal.valid, bs=bs, batch_tfms=[TSStandardize()], num_workers=0)\n",
    "dls_valence = TSDataLoaders.from_dsets(dsets_valence.train, dsets_valence.valid, bs=bs, batch_tfms=[TSStandardize()], num_workers=0)\n",
    "\n",
    "model_stress = InceptionTime(dls_stress.vars, dls_stress.c)\n",
    "model_arousal = InceptionTime(dls_arousal.vars, dls_arousal.c)\n",
    "model_valence = InceptionTime(dls_valence.vars, dls_valence.c)\n",
    "\n",
    "learn_stress = Learner(dls_stress, model_stress, metrics=accuracy)\n",
    "learn_arousal = Learner(dls_arousal, model_arousal, metrics=accuracy)\n",
    "learn_valence = Learner(dls_valence, model_valence, metrics=accuracy)\n",
    "\n",
    "learn_stress.save('S0')\n",
    "learn_arousal.save('S0')\n",
    "learn_valence.save('S0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "SuggestedLRs(valley=0.0010000000474974513)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_stress.load('S0')\n",
    "learn_stress.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.788600</td>\n      <td>1.996881</td>\n      <td>0.267264</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.438737</td>\n      <td>1.815022</td>\n      <td>0.289931</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.239650</td>\n      <td>1.659536</td>\n      <td>0.231945</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.114837</td>\n      <td>1.451738</td>\n      <td>0.366368</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.038356</td>\n      <td>1.569037</td>\n      <td>0.178703</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.996895</td>\n      <td>1.389922</td>\n      <td>0.232999</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.973628</td>\n      <td>1.416895</td>\n      <td>0.348445</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.954934</td>\n      <td>1.341174</td>\n      <td>0.230891</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.933404</td>\n      <td>1.515587</td>\n      <td>0.286241</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.914603</td>\n      <td>1.315361</td>\n      <td>0.331049</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.880976</td>\n      <td>1.348639</td>\n      <td>0.372167</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.808682</td>\n      <td>1.506323</td>\n      <td>0.340011</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.754196</td>\n      <td>1.758528</td>\n      <td>0.400105</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.706680</td>\n      <td>1.559225</td>\n      <td>0.263047</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.637212</td>\n      <td>1.164941</td>\n      <td>0.375857</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.620170</td>\n      <td>1.712813</td>\n      <td>0.344755</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.590935</td>\n      <td>1.473341</td>\n      <td>0.373221</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.582663</td>\n      <td>2.365288</td>\n      <td>0.261993</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.560015</td>\n      <td>1.315405</td>\n      <td>0.387454</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.545875</td>\n      <td>1.301018</td>\n      <td>0.370058</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.501858</td>\n      <td>1.482044</td>\n      <td>0.361624</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.506036</td>\n      <td>1.871935</td>\n      <td>0.368477</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.480566</td>\n      <td>1.759294</td>\n      <td>0.379547</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.488748</td>\n      <td>1.932134</td>\n      <td>0.282551</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.449559</td>\n      <td>1.643341</td>\n      <td>0.371112</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.431272</td>\n      <td>2.818604</td>\n      <td>0.375329</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.414669</td>\n      <td>1.200800</td>\n      <td>0.443859</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.427811</td>\n      <td>1.794697</td>\n      <td>0.390090</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.417783</td>\n      <td>2.795031</td>\n      <td>0.394834</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.383033</td>\n      <td>1.922757</td>\n      <td>0.300474</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.373999</td>\n      <td>1.629073</td>\n      <td>0.321033</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.372020</td>\n      <td>1.982120</td>\n      <td>0.342646</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.365718</td>\n      <td>1.858274</td>\n      <td>0.301002</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.340485</td>\n      <td>1.596416</td>\n      <td>0.445967</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.332403</td>\n      <td>2.408851</td>\n      <td>0.294149</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.325323</td>\n      <td>1.678756</td>\n      <td>0.488139</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.310127</td>\n      <td>1.666420</td>\n      <td>0.475488</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.308782</td>\n      <td>1.999130</td>\n      <td>0.392725</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.286421</td>\n      <td>2.845626</td>\n      <td>0.366368</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.299030</td>\n      <td>1.678402</td>\n      <td>0.446494</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.291280</td>\n      <td>1.939966</td>\n      <td>0.432261</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.283031</td>\n      <td>2.157869</td>\n      <td>0.376384</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.265279</td>\n      <td>2.112036</td>\n      <td>0.450712</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.268700</td>\n      <td>2.627437</td>\n      <td>0.379547</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.261147</td>\n      <td>2.238112</td>\n      <td>0.408540</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.248313</td>\n      <td>1.871997</td>\n      <td>0.373221</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.246763</td>\n      <td>2.830585</td>\n      <td>0.387981</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.233036</td>\n      <td>2.738903</td>\n      <td>0.381128</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.239031</td>\n      <td>2.947273</td>\n      <td>0.405904</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.237967</td>\n      <td>2.816396</td>\n      <td>0.355298</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.223546</td>\n      <td>2.294239</td>\n      <td>0.380074</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.222588</td>\n      <td>2.939766</td>\n      <td>0.393780</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.221443</td>\n      <td>3.527548</td>\n      <td>0.345809</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.215170</td>\n      <td>2.390881</td>\n      <td>0.415920</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.205887</td>\n      <td>2.271381</td>\n      <td>0.447549</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.208900</td>\n      <td>3.199303</td>\n      <td>0.319452</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.193378</td>\n      <td>3.337608</td>\n      <td>0.384818</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.203039</td>\n      <td>2.913692</td>\n      <td>0.373748</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.188909</td>\n      <td>2.511647</td>\n      <td>0.406958</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.179310</td>\n      <td>2.730353</td>\n      <td>0.383764</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.171209</td>\n      <td>2.740572</td>\n      <td>0.381655</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.177587</td>\n      <td>2.808441</td>\n      <td>0.384291</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.163999</td>\n      <td>2.943607</td>\n      <td>0.414866</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>0.165477</td>\n      <td>2.702558</td>\n      <td>0.411703</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.154804</td>\n      <td>3.396705</td>\n      <td>0.383764</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.150004</td>\n      <td>2.852568</td>\n      <td>0.374802</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.158078</td>\n      <td>3.080480</td>\n      <td>0.380074</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.156912</td>\n      <td>2.910966</td>\n      <td>0.381128</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.138971</td>\n      <td>3.014665</td>\n      <td>0.383237</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.140989</td>\n      <td>3.747772</td>\n      <td>0.387981</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.132559</td>\n      <td>2.644117</td>\n      <td>0.384291</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.134091</td>\n      <td>3.024026</td>\n      <td>0.382182</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.122869</td>\n      <td>3.269779</td>\n      <td>0.402214</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.122152</td>\n      <td>3.656959</td>\n      <td>0.414338</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.122293</td>\n      <td>3.703803</td>\n      <td>0.404850</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.120038</td>\n      <td>4.228737</td>\n      <td>0.379547</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.115652</td>\n      <td>3.201415</td>\n      <td>0.382710</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.110562</td>\n      <td>3.476210</td>\n      <td>0.382182</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>0.115646</td>\n      <td>3.948050</td>\n      <td>0.381128</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.102156</td>\n      <td>3.637710</td>\n      <td>0.383237</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.099860</td>\n      <td>3.700192</td>\n      <td>0.395361</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.098028</td>\n      <td>3.896937</td>\n      <td>0.376911</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.088865</td>\n      <td>3.792241</td>\n      <td>0.375329</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.093958</td>\n      <td>3.608576</td>\n      <td>0.384818</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.090570</td>\n      <td>3.857886</td>\n      <td>0.379547</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.088559</td>\n      <td>3.773497</td>\n      <td>0.380074</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>0.081354</td>\n      <td>3.604833</td>\n      <td>0.386927</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>0.078527</td>\n      <td>3.738519</td>\n      <td>0.381128</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>0.077737</td>\n      <td>3.845887</td>\n      <td>0.384818</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>0.075053</td>\n      <td>3.972414</td>\n      <td>0.380074</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.074599</td>\n      <td>3.820081</td>\n      <td>0.379547</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>0.072817</td>\n      <td>3.956837</td>\n      <td>0.377438</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>0.068101</td>\n      <td>3.983713</td>\n      <td>0.379019</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>0.072829</td>\n      <td>4.079136</td>\n      <td>0.377438</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>0.070507</td>\n      <td>4.051244</td>\n      <td>0.378492</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.068157</td>\n      <td>3.837630</td>\n      <td>0.379547</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>0.069598</td>\n      <td>3.906366</td>\n      <td>0.379019</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.069001</td>\n      <td>4.139438</td>\n      <td>0.374275</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>0.070428</td>\n      <td>3.938202</td>\n      <td>0.380074</td>\n      <td>00:09</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>0.067913</td>\n      <td>3.955072</td>\n      <td>0.379019</td>\n      <td>00:09</td>\n    </tr>\n  </tbody>\n</table>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Path('models/S1.pth')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_stress.fit_one_cycle(100,lr_max=1e-3)\n",
    "learn_stress.save('S1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "name": "python385jvsc74a57bd0906877fa3b0a2f49e197f8df9a8e615c458bb20b42717d7901a87c548a873a9a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "906877fa3b0a2f49e197f8df9a8e615c458bb20b42717d7901a87c548a873a9a"
   }
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}