{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "from decimal import Decimal\n",
    "import data_preprocessing\n",
    "import mlxtend\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "import utils\n",
    "import tensorflow as tf\n",
    "from decimal import Decimal\n",
    "import model\n",
    "from biosppy.signals import tools as tools\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the data- need only one time for creating the swell dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# done cause every time I change data_preprocessing.py it didn't affect\n",
    "import data_preprocessing\n",
    "import model\n",
    "import utils\n",
    "import importlib\n",
    "\n",
    "importlib.reload(data_preprocessing)\n",
    "importlib.reload(model)\n",
    "importlib.reload(utils)\n",
    "\n",
    "data_folder = os.path.join(os.path.dirname(\"data_folder\"), 'swell_dataset')\n",
    "data_preprocessing.extract_swell_dataset(overlap_pct= 0, window_size_sec= 10, data_save_path= data_folder,save= 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run SSL Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###again after script...\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Aug  8 23:57:24 2019\n",
    "\n",
    "@author: Pritam\n",
    "\"\"\"\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import model\n",
    "import utils\n",
    "import data_preprocessing\n",
    "import importlib\n",
    "importlib.reload(data_preprocessing)\n",
    "importlib.reload(model)\n",
    "importlib.reload(utils)\n",
    " \n",
    "## mention paths\n",
    "dirname = \".\"\n",
    "data_folder     = os.path.join(os.path.dirname(dirname), 'data_folder')\n",
    "summaries       = os.path.join(os.path.dirname(dirname), 'summaries')\n",
    "output          = os.path.join(os.path.dirname(dirname), 'output')\n",
    "model_dir       = os.path.join(os.path.dirname(dirname), 'models')\n",
    "swell_data = data_preprocessing.load_data(os.path.join(\"swell_dataset\",\"swell_dict.npy\"))\n",
    "\n",
    "#clean after last run\n",
    "data_preprocessing.clean_last_model()\n",
    "data_preprocessing.clean_dir('./output/ER/')\n",
    "data_preprocessing.clean_dir('./output/STR_loss/')\n",
    "data_preprocessing.clean_dir('./output/STR_result/')\n",
    "\n",
    "## transformation task params\n",
    "noise_param = 15 #noise_amount\n",
    "scale_param = 1.1 #scaling_factor\n",
    "permu_param = 20 #permutation_pieces\n",
    "tw_piece_param = 9 #time_warping_pieces\n",
    "twsf_param = 1.05 #time_warping_stretch_factor\n",
    "no_of_task = ['original_signal', 'noised_signal', 'scaled_signal', 'negated_signal', 'flipped_signal', 'permuted_signal', 'time_warped_signal'] \n",
    "transform_task = [0, 1, 2, 3, 4, 5, 6] #transformation labels\n",
    "single_batch_size = len(transform_task)\n",
    "\n",
    "## hyper parameters\n",
    "batchsize = 128  \n",
    "actual_batch_size =  batchsize * single_batch_size\n",
    "log_step = 100\n",
    "epoch = 100\n",
    "initial_learning_rate = 0.001\n",
    "drop_rate = 0.6\n",
    "regularizer = 1\n",
    "L2 = 0.0001\n",
    "lr_decay_steps = 10000\n",
    "lr_decay_rate = 0.9\n",
    "loss_coeff = [0.195, 0.195, 0.195, 0.0125, 0.0125, 0.195, 0.195]\n",
    "window_size = 2560\n",
    "extract_data = 0\n",
    "current_time    = utils.current_time()\n",
    "\n",
    "## prepared as 10 fold cv\n",
    "swell_data              = data_preprocessing.swell_prepare_for_10fold(swell_data)  #person, y_input_stress, y_arousal, y_valence, \n",
    "a = (np.where(np.isinf(swell_data)))\n",
    "b = (np.where(np.isneginf(swell_data)))\n",
    "c = (np.where(np.isnan(swell_data)))\n",
    "total_fold = 10\n",
    "kf = KFold(n_splits=total_fold, shuffle=True, random_state=True)\n",
    "swell_train_index, swell_test_index     = utils.get_train_test_index(swell_data, kf)\n",
    "\n",
    "\"\"\" self supervised task start \"\"\"\n",
    "\n",
    "graph = tf.Graph()\n",
    "print('creating graph...')\n",
    "with graph.as_default():\n",
    "    \n",
    "    ## initialize tensor\n",
    "    input_tensor        = tf.compat.v1.placeholder(tf.float32, shape = (None, window_size, 1), name = \"input\")\n",
    "    y                   = tf.compat.v1.placeholder(tf.float32, shape = (None, np.shape(transform_task)[0]), name = \"output\") \n",
    "    drop_out            = tf.compat.v1.placeholder_with_default(1.0, shape=(), name=\"Drop_out\")\n",
    "    isTrain             = tf.compat.v1.placeholder(tf.bool, name = 'isTrain')\n",
    "    global_step         = tf.Variable(0, dtype=np.float32, trainable=False, name=\"steps\")\n",
    "\n",
    "    conv1, conv2, conv3, main_branch, task_0, task_1, task_2, task_3, task_4, task_5, task_6 = model.self_supervised_model(input_tensor, isTraining= isTrain, drop_rate= drop_out)\n",
    "    logits = [task_0, task_1, task_2, task_3, task_4, task_5, task_6]\n",
    "    ## main branch is the output after all conv layers\n",
    "    featureset_size = main_branch.get_shape()[1]\n",
    "    y_label = utils.get_label(y= y, actual_batch_size= actual_batch_size)\n",
    "    all_loss = utils.calculate_loss(y_label, logits)\n",
    "    output_loss = utils.get_weighted_loss(loss_coeff, all_loss)  \n",
    "    \n",
    "    if regularizer:\n",
    "        l2_loss = 0\n",
    "        weights = []\n",
    "        for v in tf.compat.v1.trainable_variables():\n",
    "            weights.append(v)\n",
    "            if 'kernel' in v.name:\n",
    "                l2_loss += tf.nn.l2_loss(v)\n",
    "        output_loss = output_loss + l2_loss * L2\n",
    "        \n",
    "    y_pred                = utils.get_prediction(logits = logits)\n",
    "    learning_rate         = tf.compat.v1.train.exponential_decay(initial_learning_rate, global_step, decay_steps=lr_decay_steps, decay_rate=lr_decay_rate, staircase=True)\n",
    "\n",
    "    optimizer             = tf.compat.v1.train.AdamOptimizer(learning_rate) \n",
    "    \n",
    "    with tf.control_dependencies(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)):\n",
    "        train_op    = optimizer.minimize(output_loss, global_step)\n",
    "        \n",
    "    with tf.compat.v1.variable_scope('Session_saver'):\n",
    "        saver       = tf.compat.v1.train.Saver(max_to_keep=10)\n",
    "\n",
    "    tf.compat.v1.summary.scalar('learning_rate/lr', learning_rate)\n",
    "    tf.compat.v1.summary.scalar('loss/training_batch_loss', output_loss)\n",
    "    \n",
    "    summary_op      = tf.compat.v1.summary.merge_all()    \n",
    "        \n",
    "print('graph creation finished')\n",
    "\n",
    "\"\"\" Training \"\"\"\n",
    "#only one iteration for checking\n",
    "#for k in range(total_fold):\n",
    "for k in range(1): \n",
    "    flag                    = k\n",
    "    ## save STR results\n",
    "    tr_ssl_result_filename  =  os.path.join(output, \"STR_result\"   , str(\"tr_\" + str(k) +\"_\"  + current_time + \".npy\"))\n",
    "    te_ssl_result_filename  =  os.path.join(output, \"STR_result\"   , str(\"te_\" + str(k) +\"_\"  + current_time + \".npy\"))\n",
    "    tr_ssl_loss_filename    =  os.path.join(output, \"STR_loss\"     , str(\"tr_\" + str(k) +\"_\"  + current_time + \".npy\"))\n",
    "    te_ssl_loss_filename    =  os.path.join(output, \"STR_loss\"     , str(\"te_\" + str(k) +\"_\"  + current_time + \".npy\"))\n",
    "            \n",
    "    str_logs        = os.path.join(summaries, \"STR\", current_time)\n",
    "    er_logs         = os.path.join(summaries, \"ER\", current_time)\n",
    "    utils.makedirs(str_logs)\n",
    "    \n",
    "    ## combine all ECG data\n",
    "    train_ECG   = np.vstack((swell_data[swell_train_index[k], 4:])) \n",
    "    test_ECG    = np.vstack((swell_data[swell_test_index[k], 4:])) \n",
    "    train_ECG   = shuffle(train_ECG)\n",
    "    \n",
    "    ## fetch emotion recognition labels\n",
    "    train_swell_input_stress, test_swell_input_stress = utils.one_hot_encoding(arr = swell_data[:, 1], tr_index = swell_train_index[k], te_index = swell_test_index[k])\n",
    "    train_swell_arousal, test_swell_arousal           = utils.one_hot_encoding(arr = swell_data[:, 2], tr_index = swell_train_index[k], te_index = swell_test_index[k])\n",
    "    train_swell_valence, test_swell_valence           = utils.one_hot_encoding(arr = swell_data[:, 3], tr_index = swell_train_index[k], te_index = swell_test_index[k])    \n",
    "    training_length = train_ECG.shape[0]\n",
    "    testing_length  = test_ECG.shape[0]\n",
    "    \n",
    "    print('Initializing all parameters.')\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    with tf.compat.v1.Session(graph=graph) as sess:   \n",
    "        summary_writer = tf.compat.v1.summary.FileWriter(str_logs, sess.graph)\n",
    "    \n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        sess.run(tf.compat.v1.local_variables_initializer())\n",
    "        \n",
    "        print('self supervised training started')\n",
    "        \n",
    "        train_loss_dict = {}\n",
    "        test_loss_dict = {}\n",
    "    \n",
    "        tr_ssl_result = {}\n",
    "        te_ssl_result = {}    \n",
    "        \n",
    "        ## epoch loop\n",
    "        for epoch_counter in tqdm(range(epoch)):\n",
    "            \n",
    "            tr_loss_task = np.zeros((len(transform_task), 1), dtype  = np.float32)\n",
    "            train_pred_task = np.zeros((len(transform_task), actual_batch_size), dtype  = np.float32) -1\n",
    "            train_true_task = np.zeros((len(transform_task), actual_batch_size), dtype  = np.float32) -1\n",
    "            tr_output_loss = 0\n",
    "    \n",
    "            tr_total_gen_op = utils.make_total_batch(data = train_ECG, length = training_length, batchsize = batchsize, \n",
    "                                               noise_amount=noise_param, \n",
    "                                               scaling_factor=scale_param, \n",
    "                                               permutation_pieces=permu_param, \n",
    "                                               time_warping_pieces=tw_piece_param, \n",
    "                                               time_warping_stretch_factor= twsf_param, \n",
    "                                               time_warping_squeeze_factor= 1/twsf_param)\n",
    "    \n",
    "            for training_batch, training_labels, tr_counter, tr_steps in tr_total_gen_op:\n",
    "                \n",
    "                ## run the model here \n",
    "                training_batch, training_labels = utils.unison_shuffled_copies(training_batch, training_labels)\n",
    "                training_batch = training_batch.reshape(training_batch.shape[0], training_batch.shape[1], 1)\n",
    "                fetches = [all_loss, output_loss, y_pred, train_op]\n",
    "                if tr_counter % log_step == 0:\n",
    "                    fetches.append(summary_op)\n",
    "                    \n",
    "                fetched = sess.run(fetches, {input_tensor: training_batch, y: training_labels, drop_out: drop_rate, isTrain: True})\n",
    "                \n",
    "                if tr_counter % log_step == 0: # \n",
    "                    summary_writer.add_summary(fetched[-1], tr_counter)\n",
    "                    summary_writer.flush()\n",
    "    \n",
    "                tr_loss_task = utils.fetch_all_loss(fetched[0], tr_loss_task) \n",
    "                tr_output_loss += fetched[1]\n",
    "                \n",
    "                train_pred_task = utils.fetch_pred_labels(fetched[2], train_pred_task)\n",
    "                train_true_task = utils.fetch_true_labels(training_labels, train_true_task)\n",
    "\n",
    "            ## loss after epoch\n",
    "            tr_epoch_loss = np.true_divide(tr_loss_task, tr_steps)\n",
    "            train_loss_dict.update({epoch_counter: tr_epoch_loss})\n",
    "            tr_output_loss = np.true_divide(tr_output_loss, tr_steps)\n",
    "            \n",
    "            ## performance matrix after each epoch\n",
    "            tr_epoch_accuracy, tr_epoch_f1_score = utils.get_results_ssl(train_true_task, np.asarray(train_pred_task, int))\n",
    "            tr_ssl_result = utils.write_result(tr_epoch_accuracy, tr_epoch_f1_score, epoch_counter, tr_ssl_result)\n",
    "            utils.write_summary(loss = tr_epoch_loss, total_loss = tr_output_loss, f1_score = tr_epoch_f1_score, epoch_counter = epoch_counter, isTraining = True, summary_writer = summary_writer)\n",
    "            utils.write_result_csv(k, epoch_counter, os.path.join(output, \"STR_result\", \"tr_str_f1_Score.csv\"), tr_epoch_f1_score)\n",
    "    \n",
    "            model_path = os.path.join(model_dir , \"epoch_\" + str(epoch_counter))\n",
    "            utils.makedirs(model_path)\n",
    "            save_path = saver.save(sess, os.path.join(model_path, \"SSL_model.ckpt\"))\n",
    "            print(\"Self-supervised trained model is saved in path: %s\" % save_path) \n",
    "            \n",
    "            ## initialize array\n",
    "            te_loss_task    = np.zeros((len(transform_task), 1), dtype  = np.float32)\n",
    "            test_pred_task  = np.zeros((len(transform_task), actual_batch_size), dtype  = np.float32)-1\n",
    "            test_true_task  = np.zeros((len(transform_task), actual_batch_size), dtype  = np.float32)-1\n",
    "            te_output_loss  = 0\n",
    "           \n",
    "            te_total_gen_op = utils.make_total_batch(data = test_ECG, \n",
    "                                                     length = testing_length, \n",
    "                                                     batchsize = batchsize, \n",
    "                                                     noise_amount=noise_param, \n",
    "                                                     scaling_factor=scale_param, \n",
    "                                                     permutation_pieces=permu_param, \n",
    "                                                     time_warping_pieces=tw_piece_param, \n",
    "                                                     time_warping_stretch_factor= twsf_param, \n",
    "                                                     time_warping_squeeze_factor= 1/twsf_param)\n",
    "    \n",
    "            for testing_batch, testing_labels, te_counter, te_steps in te_total_gen_op:\n",
    "                \n",
    "                ## run the model here \n",
    "                fetches = [all_loss, output_loss, y_pred]\n",
    "                    \n",
    "                #fetched = sess.run(fetches, {input_tensor: testing_batch, y: testing_labels, drop_out: 0.0, isTrain: False})\n",
    "                #try:\n",
    "                fetched = sess.run(fetches, {input_tensor: testing_batch, y: testing_labels, drop_out: 0.5, isTrain: False})\n",
    "    \n",
    "                te_loss_task = utils.fetch_all_loss(fetched[0], te_loss_task)\n",
    "                te_output_loss += fetched[1]\n",
    "                test_pred_task = utils.fetch_pred_labels(fetched[2], test_pred_task)\n",
    "                test_true_task = utils.fetch_true_labels(testing_labels, test_true_task)\n",
    "    \n",
    "            ## loss after epoch\n",
    "            te_epoch_loss = np.true_divide(te_loss_task, te_steps)\n",
    "            test_loss_dict.update({epoch_counter: te_epoch_loss})\n",
    "            te_output_loss = np.true_divide(te_output_loss, te_steps)\n",
    "    \n",
    "            ## performance matrix after each epoch\n",
    "            te_epoch_accuracy, te_epoch_f1_score = utils.get_results_ssl(test_true_task, test_pred_task)            \n",
    "            te_ssl_result = utils.write_result(te_epoch_accuracy, te_epoch_f1_score, epoch_counter, te_ssl_result)    \n",
    "            utils.write_summary(loss = te_epoch_loss, total_loss = te_output_loss, f1_score = te_epoch_f1_score, epoch_counter = epoch_counter, isTraining = False, summary_writer = summary_writer)\n",
    "            utils.write_result_csv(k, epoch_counter, os.path.join(output, \"STR_result\", \"te_str_f1_score.csv\"), te_epoch_f1_score)\n",
    "            \n",
    "            \"\"\"\n",
    "            supervised task of self supervised learning\n",
    "            \"\"\"\n",
    "            \"\"\"  swell \"\"\"\n",
    "               \n",
    "            ## training - testing ECG\n",
    "            x_tr = swell_data[swell_train_index[k], 4:]\n",
    "            x_te = swell_data[swell_test_index[k], 4:]\n",
    "                \n",
    "            ## features extracted from conv layers\n",
    "            x_tr_feature = utils.extract_feature(x_original = x_tr, featureset_size = featureset_size, batch_super = batchsize, input_tensor = input_tensor, isTrain = isTrain, drop_out = drop_out, extract_layer = main_branch, sess = sess)\n",
    "            x_te_feature = utils.extract_feature(x_original = x_te, featureset_size = featureset_size, batch_super = batchsize, input_tensor = input_tensor, isTrain = isTrain, drop_out = drop_out, extract_layer = main_branch, sess = sess)\n",
    "                \n",
    "            ## supervised emotion recognition\n",
    "            model.supervised_model_swell(x_tr_feature = x_tr_feature, y_tr = train_swell_input_stress, x_te_feature = x_te_feature, y_te = test_swell_input_stress, identifier = 'swell_input_stress', kfold = flag, result = output, summaries = er_logs, current_time = current_time)        \n",
    "            model.supervised_model_swell(x_tr_feature = x_tr_feature, y_tr = train_swell_arousal, x_te_feature = x_te_feature, y_te = test_swell_arousal, identifier = 'swell_arousal', kfold = flag, result = output, summaries = er_logs, current_time = current_time)  \n",
    "            model.supervised_model_swell(x_tr_feature = x_tr_feature, y_tr = train_swell_valence, x_te_feature = x_te_feature, y_te = test_swell_valence, identifier = 'swell_valence', kfold = flag, result = output, summaries = er_logs, current_time = current_time)  \n",
    "           \n",
    "        ## save str loss, acc and f1 score    \n",
    "        np.save(tr_ssl_loss_filename, train_loss_dict)\n",
    "        np.save(te_ssl_loss_filename, test_loss_dict)\n",
    "    \n",
    "        np.save(tr_ssl_result_filename, tr_ssl_result)\n",
    "        np.save(te_ssl_result_filename, te_ssl_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e3840fbafd1ea49c623d07301d22bc596de915b4ad3d49db03b5731eabfa3f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
